<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Speech Synthesis by FlorianLux</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Speech Synthesis</h1>
        <p>Toolkit to train and experiment with state-of-the-art Speech Synthesis models.</p>

        <p class="view"><a href="https://github.tik.uni-stuttgart.de/FlorianLux/SpeechSynthesis">View the Project on GitHub <small>FlorianLux/SpeechSynthesis</small></a></p>


      </header>
      <section>
        <p>This is a toolkit to train state-of-the-art Speech Synthesis models. Everything is pure Python and PyTorch based to keep
it as simple and beginner-friendly, yet powerful as possible.</p>
<p>The PyTorch Modules of <a href="https://arxiv.org/abs/1809.08895">TransformerTTS</a>
and <a href="https://arxiv.org/abs/2006.04558">FastSpeech2</a> are taken from <a href="https://github.com/espnet/espnet">ESPnet</a>, the
PyTorch Modules of <a href="https://arxiv.org/abs/1910.06711">MelGAN</a> are taken from
the <a href="https://github.com/kan-bayashi/ParallelWaveGAN">ParallelWaveGAN repository</a> which are also authored by the
brillant <a href="https://github.com/kan-bayashi">Tomoki Hayashi</a>.</p>
<h2>
<a id="demonstration" class="anchor" href="#demonstration" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demonstration</h2>
<p><a href="https://drive.google.com/file/d/1mZ1LvTlY6pJ5ZQ4UXZ9jbzB651mufBrB/view?usp=sharing">Here is some speech</a> produced by
FastSpeech 2 and MelGAN trained on <a href="https://keithito.com/LJ-Speech-Dataset/">LJSpeech</a> using this toolkit.</p>
<p>And <a href="https://drive.google.com/file/d/1FT49Jf0yyibwMDbsEJEO9mjwHkHRIGXc/view?usp=sharing">here is a sentence</a> produced by
TransformerTTS and MelGAN trained on <a href="https://github.com/thorstenMueller/deep-learning-german-tts">Thorsten</a> using this
toolkit.</p>
<p><a href="https://drive.google.com/file/d/14nPo2o1VKtWLPGF7e_0TxL8XGI3n7tAs/view?usp=sharing">Here is some speech</a> produced by a
multi-speaker FastSpeech 2 with MelGAN trained on <a href="https://research.google/tools/datasets/libri-tts/">LibriTTS</a> using
this toolkit. Fans of the videogame Portal may recognize who was used as the reference speaker for this utterance.</p>
<h2>
<a id="embrace-redundancy" class="anchor" href="#embrace-redundancy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Embrace Redundancy</h2>
<p>While it is a bad practice to have redundancy in regular code, and as much as possible should be abstracted and
parameterized, my experiences in creating this toolkit and working with other toolkits led me to believe that sometimes
redundancy is not only ok, it is actually very convenient. While it does make it more difficult to change things, it
also makes it also more difficult to break things and cause legacy problems.</p>
<hr>
<h2>
<a id="working-with-this-toolkit" class="anchor" href="#working-with-this-toolkit" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Working with this Toolkit</h2>
<p>The standard way of working with this toolkit is to make your own fork of it, so you can change as much of the code as
you like and fully adapt it to your needs. Making pipelines to train models on new datasets, even in new languages,
requires absolutely minimal new code and you can take the existing code for such models as reference/template.</p>
<h2>
<a id="installation" class="anchor" href="#installation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>
<p>To install this toolkit, clone it onto the machine you want to use it on (should have at least one GPU if you intend to
train models on that machine. For inference you can get by without GPU). Navigate to the directory you have cloned and
run the command shown below. It is recommended to first create and activate
a <a href="https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/#creating-a-virtual-environment">pip virtual environment</a>
.</p>
<pre><code>pip install -r requirements.txt 
</code></pre>
<p>If you want to use multi-speaker synthesis, you will need a speaker embedding function. The one assumed in the code
is <a href="https://github.com/yistLin/dvector">dvector</a>, because it is incredibly easy to use and freely available. Create a
directory <em>Models</em> in the top-level of your clone. Then create a directory <em>Use</em> in there and in this directory create
another directory called <em>SpeakerEmbedding</em>. In this directory you put the two files <em>wav2mel.pt</em> and
<em>dvector-step250000.pt</em> that you can obtain from the release page of the <a href="https://github.com/yistLin/dvector">dvector</a>
GitHub. This process might become automated in the future.</p>
<p>And finally you need to have espeak installed on your system, because it is used as backend for the phonemizer. If you
replace the phonemizer, you don't need it. On most Linux environments it will be installed already, and if it is not and
you have the sufficient rights you can install it by simply running</p>
<pre><code>apt-get install espeak
</code></pre>
<h2>
<a id="creating-a-new-pipeline" class="anchor" href="#creating-a-new-pipeline" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Creating a new Pipeline</h2>
<p>To create a new pipeline to train a MelGAN vocoder, you only need a set of audio files. To create a new pipeline for a
TransformerTTS you need audio files and corresponding text labels. To create a new pipeline for a FastSpeech 2, you need
audio files, corresponding text labels, and an already trained TransformerTTS to estimate the duration information that
FastSpeech 2 needs as input. Let's go through them in order of increasing complexity.</p>
<h4>
<a id="build-a-melgan-pipeline" class="anchor" href="#build-a-melgan-pipeline" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build a MelGAN Pipeline</h4>
<p>In the directory called <em>Utility</em> there is a file called <em>file_lists.py</em>. In this file you should write a function that
returns a list of all of the absolute paths to each of the audio files in your dataset as strings.</p>
<p>Then go to the directory <em>Pipelines</em> from the top level of the toolkit. In there, make a copy of any existing pipeline
that has MelGAN in its name. We will use this as reference and only make the necessary changes to use the new dataset.
Import the function you have just written as <em>get_file_list</em>. Now look out for a variable called <em>model_save_dir</em>. This
is the default directory that checkpoints will be saved into, unless you specify another one when calling the training
script. Change it to whatever you like.</p>
<p>Now you need to add your newly created pipeline to the the pipeline dictionary in the file <em>run_training_pipeline.py</em> in
the top level of the toolkit. In this file, import the <em>run</em> function from the pipeline you just created and give it a
speaking name. Now in the <em>pipeline_dict</em>, add your imported function as value and use as key a shorthand that makes
sense. And just like that you're done.</p>
<h4>
<a id="build-a-transformertts-pipeline" class="anchor" href="#build-a-transformertts-pipeline" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build a TransformerTTS Pipeline</h4>
<p>In the directory called <em>Utility</em> there is a file called <em>path_to_transcript_dicts.py</em>. In this file you should write a
function that returns a dictionary that has all of the absolute paths to each of the audio files in your dataset as
strings as the keys and the textual transcriptions of the corresponding audios as the values.</p>
<p>Then go to the directory <em>Pipelines</em> from the top level of the toolkit. In there, make a copy of any existing pipeline
that has TransformerTTS in its name. If your dataset is single-speaker, choose any that is not LibriTTS. If your dataset
is multi-speaker, choose the one for LibriTTS as your template. We will use this copy as reference and only make the
necessary changes to use the new dataset. Import the function you have just written as <em>build_path_to_transcript_dict</em>.
Since the data will be processed a considerable amount, a cache will be built and saved as file for quick and easy
restarts. So find the variable <em>cache_dir</em> and adapt it to your needs. The same goes for the variable <em>save_dir</em>, which
is where the checkpoints will be saved to. This is a default value, you can overwrite it when calling the pipeline later
using a command line argument, in case you want to fine-tune from a checkpoint and thus save into a different directory.</p>
<p>Since we are using text here, we have to make sure that the text processing is adequate for the language. So check
in <em>PreprocessingForTTS/ProcessText</em> whether the TextFrontend already has a language ID (e.g. 'en' and 'de') for the
language of your dataset. If not, you'll have to implement handling for that, but it should be pretty simple by just
doing it analogous to what is there already. Now back in the pipeline, change the <em>lang</em> argument in the creation of the
dataset and in the call to the train loop function to the language ID that matches your data.</p>
<p>Now navigate to the implementation of the <em>train_loop</em> that is called in the pipeline. In this file, find the function
called <em>get_atts</em>. This function will produce attention plots during training, which is the most important way to
monitor the progress of the training. In there, you may need to add an example sentence for the language of the data you
are using. It should all be pretty clear from looking at it.</p>
<p>Once this is done, we are almost done, now we just need to make it available to the <em>run_training_pipeline.py</em> file in
the top level. In said file, import the <em>run</em> function from the pipeline you just created and give it a speaking name.
Now in the <em>pipeline_dict</em>, add your imported function as value and use as key a shorthand that makes sense. And that's
it.</p>
<h4>
<a id="build-a-fastspeech-2-pipeline" class="anchor" href="#build-a-fastspeech-2-pipeline" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build a FastSpeech 2 Pipeline</h4>
<p>Most of this is exactly analogous to building a TransformerTTS pipeline. So to keep this brief, this section will only
mention the additional things you have to do.</p>
<p>In your new pipeline file, look out for the line in which the <em>acoustic_model</em> is loaded. Change the path to the
checkpoint of a TransformerTTS model that you trained on the same dataset previously. Then look out for the creation of
the <em>train_set</em>. In there, there is an argument called <em>diagonal_attention_head_id</em>.</p>
<p>It is recommended to use an <em>InferenceInterface</em> of the aforementioned TransformerTTS model to determine which of the
attention heads looks the most like a duration graph. How to make one is described in a later section. To determine the
attention head to use, add the <em>InferenceInterface</em> to the dictionary in the <em>view_attention_heads</em> function in
the <em>run_visualizations.py</em> file in the top level of the toolkit. Then call it to see a plot of all of the attention heads
visualized with their ID displayed above them. This ID is what you want to supply to the <em>diagonal_attention_head_id</em>
argument in the pipeline as an integer. If you use the default argument (<em>None</em>) it will try to select the most diagonal
head for each sample automatically, but this fails for some samples, so it is safer to do it manually.</p>
<p>Everything else is exactly like creating a TransformerTTS pipeline, except that in the training_loop, instead of
attentions plots, spectrograms are plotted to visualize training progress. So there you may need to add a sentence if
you are using a new language in the function called <em>plot_progress_spec</em>.</p>
<h2>
<a id="training-a-model" class="anchor" href="#training-a-model" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Training a Model</h2>
<p>Once you have a pipeline built, training is super easy. Just activate your virtual environment and run the command
below. You might want to use something like nohup to keep it running after you log out from the server (then you should
also add -u as option to python) and add an &amp; to start it in the background. Also you might want to direct the std:out
and std:err into a file using &gt; but all of that is just standard shell use and has nothing to do with the toolkit.</p>
<pre><code>python run_training_pipeline.py &lt;shorthand of the pipeline&gt;
</code></pre>
<p>You can supply any of the following arguments, but don't have to (although for training you should definitely specify at
least a GPU ID).</p>
<pre><code>--gpu_id &lt;ID of the GPU you wish to use, as displayed with nvidia-smi, default is cpu&gt; 

--resume_checkpoint &lt;path to a checkpoint to load&gt;

--finetune (if this is present, the provided checkpoint will be fine-tuned on the data from this pipeline)

--model_save_dir &lt;path to a directory where the checkpoints should be saved&gt;
</code></pre>
<p>After every epoch, some logs will be written to the console. If the loss becomes NaN, you'll need to use a smaller
learning rate or more warmup steps in the arguments of the call to the training_loop in the pipeline you are running.</p>
<p>If you get cuda out of memory errors, you need to decrease the batchsize in the arguments of the call to the
training_loop in the pipeline you are running. You can compensate for the smaller batchsize by increasing the
gradient_accumulation argument. This will cause an update to the weigths to only happen after the specified amount of
batches. This is important for training stability, but makes training a lot slower. So try decreasing the batchsize
until you get no more out of cuda memory errors, and only increase the gradient_accumulation if training fails then due
to the smaller batch size (i.e. the plots don't show improvements after a few hours).</p>
<p>Speaking of plots: in the directory you specified for saving models checkpoint files and self explanatory visualization
data will appear. Since the checkpoints are quite big, only the five most recent ones will be kept. Training will stop
after 300,000 update steps have been made by default for TransformerTTS and FastSpeech2, and after 500,000 steps for
MelGAN. Depending on the machine and configuration you are using this will take between 2 and 4 days, so verify that
everything works on small tests before running the big thing. If you want to stop earlier, just kill the process, since
everything is daemonic all of the child-processes should die with it.</p>
<p>After training is complete, it is recommended to run <em>run_weight_averaging.py</em>. If you made no changes to the
architectures and stuck to the default directory layout, it will automatically load any models you produced with one
pipeline, average their parameters to get a slightly more robust model and save the result as <em>best.pt</em> in the same
directory where all of the corresponding checkpoints lie. This also compresses the file size slightly, so you should do
this and then use the <em>best.pt</em> model for inference.</p>
<h2>
<a id="creating-a-new-inferenceinterface" class="anchor" href="#creating-a-new-inferenceinterface" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Creating a new InferenceInterface</h2>
<p>To build a new <em>InferenceInterface</em>, which you can then use for super simple inference, we're going to use an existing
one as template again. If you use multi-speaker, take the LibriTTS ones as template, otherwise take any other one. Make
a copy of the <em>InferenceInterface</em>. Change the name of the class in the copy and change the paths to the models to use
the trained models of your choice. Instantiate the model with the same hyperparameters that you used when you created it
in the corresponding training pipeline. The last thing to check is the language that you supply to the text frontend.
Make sure it matches what you used during training.</p>
<p>With your newly created <em>InferenceInterface</em>, you can use your trained models pretty much anywhere, e.g. in other
projects. All you need is the <em>Utility</em> directory, the <em>Layers</em> directory, the <em>PreprocessingForTTS</em> directory and
the <em>InferenceInterfaces</em> directory (and of course your model checkpoint). That's all the code you need, it works standalone.</p>
<h2>
<a id="using-a-trained-model-for-inference" class="anchor" href="#using-a-trained-model-for-inference" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Using a trained Model for Inference</h2>
<p>An <em>InferenceInterface</em> of TransformerTTS contains 3 useful methods for inference, a FastSpeech 2 one contains 2 useful
methods. The ones they share are <em>read_to_file</em> and <em>read_aloud</em>.</p>
<ul>
<li>The first one takes as input a list of strings and a filename. It will synthesize the sentences in the list and
concatenate them with a short pause inbetween and write them to the filepath you supply as the other argument.</li>
<li>The second one takes just a string, which it will then convert to speech and immediately play using the system's
speakers. If you set the optional argument <em>view</em> to <em>True</em> when calling it, it will also show a plot of the phonemes
it produced, the spectrogram it came up with and the wave it created from that spectrogram. So all of the
representations can be seen, text to phoneme, phoneme to spectrogram and finally spectrogram to wave.</li>
<li>The one that is exclusive to TransformerTTS is <em>plot_attentions</em>. This will take a string, synthesize it and show a
plot of the attention matrices of all of the attention heads in all of the attention layers. By default there are 24.
The index of each head is displayed above (0-23 by default). This is useful to figure out which one of the heads
contains the best temporal relation between text and audio, which is the ID that should be given to the FastSpeech 2
feature-extraction to heuristically get duration information using a technique called <em>knowledge-distillation</em>.</li>
</ul>
<p>Those three methods are used in demo code in the toolkit. In <em>run_interactive_demo.py</em>, <em>run_text_to_file_reader.py</em>
and <em>run_visualizations.py</em> you can import <em>InferenceInterfaces</em> that you created and add them to the dictionary in each
of the files with a shorthand that makes sense. In the interactive demo, you can just call the python script, then type
in the shorthand when prompted and immediately listen to your synthesis saying whatever you put in next (be wary of out
of memory errors for too long inputs). In the other two demo scripts you have to call the function that wraps around
the <em>InferenceInterface</em> and supply the shorthand of your choice. It should be pretty clear from looking at it.</p>
<hr>
<h2>
<a id="example-pipelines-available" class="anchor" href="#example-pipelines-available" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Example Pipelines available</h2>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Language</th>
<th>Single or Multi</th>
<th align="center">MelGAN</th>
<th align="center">TransformerTTS</th>
<th align="center">FastSpeech2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hokuspokus</td>
<td>German</td>
<td>Single Speaker</td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
</tr>
<tr>
<td>Thorsten</td>
<td>German</td>
<td>Single Speaker</td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
</tr>
<tr>
<td>MAILabs Karlsson</td>
<td>German</td>
<td>Single Speaker</td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
</tr>
<tr>
<td>MAILabs Eva</td>
<td>German</td>
<td>Single Speaker</td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
</tr>
<tr>
<td>LJSpeech</td>
<td>English</td>
<td>Single Speaker</td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
</tr>
<tr>
<td>Nancy Krebs Lessac</td>
<td>English</td>
<td>Single Speaker</td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
</tr>
<tr>
<td>MAILabs Elizabeth</td>
<td>English</td>
<td>Single Speaker</td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
</tr>
<tr>
<td>LibriTTS</td>
<td>English</td>
<td>Multi Speaker</td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td align="center"><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://assets.github.tik.uni-stuttgart.de/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
</tr>
</tbody>
</table>
<hr>
<p>This toolkit has been written by Florian Lux (except for the pytorch modules taken
from <a href="https://github.com/espnet/espnet">ESPnet</a> and <a href="https://github.com/kan-bayashi/ParallelWaveGAN">ParallelWaveGAN</a>,
as mentioned above), so if you come across problems or questions, feel free to <a href="mailto:florian.lux@ims.uni-stuttgart.de">write a mail</a>. Also let me know if you do something
cool with it. Thank you for reading.</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.tik.uni-stuttgart.de/FlorianLux">FlorianLux</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
