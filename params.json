{"name":"Speech Synthesis","tagline":"Toolkit to train and experiment with state-of-the-art Speech Synthesis models.","body":"This is a toolkit to train state-of-the-art Speech Synthesis models. Everything is pure Python and PyTorch based to keep\r\nit as simple and beginner-friendly, yet powerful as possible.\r\n\r\nThe PyTorch Modules of [TransformerTTS](https://arxiv.org/abs/1809.08895)\r\nand [FastSpeech2](https://arxiv.org/abs/2006.04558) are taken from [ESPnet](https://github.com/espnet/espnet), the\r\nPyTorch Modules of [MelGAN](https://arxiv.org/abs/1910.06711) are taken from\r\nthe [ParallelWaveGAN repository](https://github.com/kan-bayashi/ParallelWaveGAN) which are also authored by the\r\nbrillant [Tomoki Hayashi](https://github.com/kan-bayashi).\r\n\r\n## Demonstration\r\n\r\n[Here is some speech](https://drive.google.com/file/d/1mZ1LvTlY6pJ5ZQ4UXZ9jbzB651mufBrB/view?usp=sharing) produced by\r\nFastSpeech 2 and MelGAN trained on [LJSpeech](https://keithito.com/LJ-Speech-Dataset/) using this toolkit.\r\n\r\nAnd [here is a sentence](https://drive.google.com/file/d/1FT49Jf0yyibwMDbsEJEO9mjwHkHRIGXc/view?usp=sharing) produced by\r\nTransformerTTS and MelGAN trained on [Thorsten](https://github.com/thorstenMueller/deep-learning-german-tts) using this\r\ntoolkit.\r\n\r\n[Here is some speech](https://drive.google.com/file/d/14nPo2o1VKtWLPGF7e_0TxL8XGI3n7tAs/view?usp=sharing) produced by a\r\nmulti-speaker FastSpeech 2 with MelGAN trained on [LibriTTS](https://research.google/tools/datasets/libri-tts/) using\r\nthis toolkit. Fans of the videogame Portal may recognize who was used as the reference speaker for this utterance.\r\n\r\n## Embrace Redundancy\r\n\r\nWhile it is a bad practice to have redundancy in regular code, and as much as possible should be abstracted and\r\nparameterized, my experiences in creating this toolkit and working with other toolkits led me to believe that sometimes\r\nredundancy is not only ok, it is actually very convenient. While it does make it more difficult to change things, it\r\nalso makes it also more difficult to break things and cause legacy problems.\r\n\r\n---\r\n\r\n## Working with this Toolkit\r\n\r\nThe standard way of working with this toolkit is to make your own fork of it, so you can change as much of the code as\r\nyou like and fully adapt it to your needs. Making pipelines to train models on new datasets, even in new languages,\r\nrequires absolutely minimal new code and you can take the existing code for such models as reference/template.\r\n\r\n## Installation\r\n\r\nTo install this toolkit, clone it onto the machine you want to use it on (should have at least one GPU if you intend to\r\ntrain models on that machine. For inference you can get by without GPU). Navigate to the directory you have cloned and\r\nrun the command shown below. It is recommended to first create and activate\r\na [pip virtual environment](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/#creating-a-virtual-environment)\r\n.\r\n\r\n```\r\npip install -r requirements.txt \r\n```\r\n\r\nIf you want to use multi-speaker synthesis, you will need a speaker embedding function. The one assumed in the code\r\nis [dvector](https://github.com/yistLin/dvector), because it is incredibly easy to use and freely available. Create a\r\ndirectory *Models* in the top-level of your clone. Then create a directory *Use* in there and in this directory create\r\nanother directory called *SpeakerEmbedding*. In this directory you put the two files *wav2mel.pt* and \r\n*dvector-step250000.pt* that you can obtain from the release page of the [dvector](https://github.com/yistLin/dvector)\r\nGitHub. This process might become automated in the future.\r\n\r\nAnd finally you need to have espeak installed on your system, because it is used as backend for the phonemizer. If you\r\nreplace the phonemizer, you don't need it. On most Linux environments it will be installed already, and if it is not and\r\nyou have the sufficient rights you can install it by simply running\r\n\r\n```\r\napt-get install espeak\r\n```\r\n\r\n## Creating a new Pipeline\r\n\r\nTo create a new pipeline to train a MelGAN vocoder, you only need a set of audio files. To create a new pipeline for a\r\nTransformerTTS you need audio files and corresponding text labels. To create a new pipeline for a FastSpeech 2, you need\r\naudio files, corresponding text labels, and an already trained TransformerTTS to estimate the duration information that\r\nFastSpeech 2 needs as input. Let's go through them in order of increasing complexity.\r\n\r\n#### Build a MelGAN Pipeline\r\n\r\nIn the directory called *Utility* there is a file called *file_lists.py*. In this file you should write a function that\r\nreturns a list of all of the absolute paths to each of the audio files in your dataset as strings.\r\n\r\nThen go to the directory *Pipelines* from the top level of the toolkit. In there, make a copy of any existing pipeline\r\nthat has MelGAN in its name. We will use this as reference and only make the necessary changes to use the new dataset.\r\nImport the function you have just written as *get_file_list*. Now look out for a variable called *model_save_dir*. This\r\nis the default directory that checkpoints will be saved into, unless you specify another one when calling the training\r\nscript. Change it to whatever you like.\r\n\r\nNow you need to add your newly created pipeline to the the pipeline dictionary in the file *run_training_pipeline.py* in\r\nthe top level of the toolkit. In this file, import the *run* function from the pipeline you just created and give it a\r\nspeaking name. Now in the *pipeline_dict*, add your imported function as value and use as key a shorthand that makes\r\nsense. And just like that you're done.\r\n\r\n#### Build a TransformerTTS Pipeline\r\n\r\nIn the directory called *Utility* there is a file called *path_to_transcript_dicts.py*. In this file you should write a\r\nfunction that returns a dictionary that has all of the absolute paths to each of the audio files in your dataset as\r\nstrings as the keys and the textual transcriptions of the corresponding audios as the values.\r\n\r\nThen go to the directory *Pipelines* from the top level of the toolkit. In there, make a copy of any existing pipeline\r\nthat has TransformerTTS in its name. If your dataset is single-speaker, choose any that is not LibriTTS. If your dataset\r\nis multi-speaker, choose the one for LibriTTS as your template. We will use this copy as reference and only make the\r\nnecessary changes to use the new dataset. Import the function you have just written as *build_path_to_transcript_dict*.\r\nSince the data will be processed a considerable amount, a cache will be built and saved as file for quick and easy\r\nrestarts. So find the variable *cache_dir* and adapt it to your needs. The same goes for the variable *save_dir*, which\r\nis where the checkpoints will be saved to. This is a default value, you can overwrite it when calling the pipeline later\r\nusing a command line argument, in case you want to fine-tune from a checkpoint and thus save into a different directory.\r\n\r\nSince we are using text here, we have to make sure that the text processing is adequate for the language. So check \r\nin *PreprocessingForTTS/ProcessText* whether the TextFrontend already has a language ID (e.g. 'en' and 'de') for the\r\nlanguage of your dataset. If not, you'll have to implement handling for that, but it should be pretty simple by just\r\ndoing it analogous to what is there already. Now back in the pipeline, change the *lang* argument in the creation of the\r\ndataset and in the call to the train loop function to the language ID that matches your data.\r\n\r\nNow navigate to the implementation of the *train_loop* that is called in the pipeline. In this file, find the function\r\ncalled *get_atts*. This function will produce attention plots during training, which is the most important way to\r\nmonitor the progress of the training. In there, you may need to add an example sentence for the language of the data you\r\nare using. It should all be pretty clear from looking at it.\r\n\r\nOnce this is done, we are almost done, now we just need to make it available to the *run_training_pipeline.py* file in\r\nthe top level. In said file, import the *run* function from the pipeline you just created and give it a speaking name.\r\nNow in the *pipeline_dict*, add your imported function as value and use as key a shorthand that makes sense. And that's\r\nit.\r\n\r\n#### Build a FastSpeech 2 Pipeline\r\n\r\nMost of this is exactly analogous to building a TransformerTTS pipeline. So to keep this brief, this section will only\r\nmention the additional things you have to do.\r\n\r\nIn your new pipeline file, look out for the line in which the *acoustic_model* is loaded. Change the path to the\r\ncheckpoint of a TransformerTTS model that you trained on the same dataset previously. Then look out for the creation of\r\nthe *train_set*. In there, there is an argument called *diagonal_attention_head_id*.\r\n\r\nIt is recommended to use an *InferenceInterface* of the aforementioned TransformerTTS model to determine which of the\r\nattention heads looks the most like a duration graph. How to make one is described in a later section. To determine the\r\nattention head to use, add the *InferenceInterface* to the dictionary in the *view_attention_heads* function in \r\nthe *run_visualizations.py* file in the top level of the toolkit. Then call it to see a plot of all of the attention heads\r\nvisualized with their ID displayed above them. This ID is what you want to supply to the *diagonal_attention_head_id*\r\nargument in the pipeline as an integer. If you use the default argument (*None*) it will try to select the most diagonal\r\nhead for each sample automatically, but this fails for some samples, so it is safer to do it manually.\r\n\r\nEverything else is exactly like creating a TransformerTTS pipeline, except that in the training_loop, instead of\r\nattentions plots, spectrograms are plotted to visualize training progress. So there you may need to add a sentence if\r\nyou are using a new language in the function called *plot_progress_spec*.\r\n\r\n## Training a Model\r\n\r\nOnce you have a pipeline built, training is super easy. Just activate your virtual environment and run the command\r\nbelow. You might want to use something like nohup to keep it running after you log out from the server (then you should\r\nalso add -u as option to python) and add an & to start it in the background. Also you might want to direct the std:out\r\nand std:err into a file using > but all of that is just standard shell use and has nothing to do with the toolkit.\r\n\r\n```\r\npython run_training_pipeline.py <shorthand of the pipeline>\r\n```\r\n\r\nYou can supply any of the following arguments, but don't have to (although for training you should definitely specify at\r\nleast a GPU ID).\r\n\r\n```\r\n--gpu_id <ID of the GPU you wish to use, as displayed with nvidia-smi, default is cpu> \r\n\r\n--resume_checkpoint <path to a checkpoint to load>\r\n\r\n--finetune (if this is present, the provided checkpoint will be fine-tuned on the data from this pipeline)\r\n\r\n--model_save_dir <path to a directory where the checkpoints should be saved>\r\n```\r\n\r\nAfter every epoch, some logs will be written to the console. If the loss becomes NaN, you'll need to use a smaller\r\nlearning rate or more warmup steps in the arguments of the call to the training_loop in the pipeline you are running.\r\n\r\nIf you get cuda out of memory errors, you need to decrease the batchsize in the arguments of the call to the\r\ntraining_loop in the pipeline you are running. You can compensate for the smaller batchsize by increasing the\r\ngradient_accumulation argument. This will cause an update to the weigths to only happen after the specified amount of\r\nbatches. This is important for training stability, but makes training a lot slower. So try decreasing the batchsize\r\nuntil you get no more out of cuda memory errors, and only increase the gradient_accumulation if training fails then due\r\nto the smaller batch size (i.e. the plots don't show improvements after a few hours).\r\n\r\nSpeaking of plots: in the directory you specified for saving models checkpoint files and self explanatory visualization\r\ndata will appear. Since the checkpoints are quite big, only the five most recent ones will be kept. Training will stop\r\nafter 300,000 update steps have been made by default for TransformerTTS and FastSpeech2, and after 500,000 steps for\r\nMelGAN. Depending on the machine and configuration you are using this will take between 2 and 4 days, so verify that\r\neverything works on small tests before running the big thing. If you want to stop earlier, just kill the process, since\r\neverything is daemonic all of the child-processes should die with it.\r\n\r\nAfter training is complete, it is recommended to run *run_weight_averaging.py*. If you made no changes to the\r\narchitectures and stuck to the default directory layout, it will automatically load any models you produced with one\r\npipeline, average their parameters to get a slightly more robust model and save the result as *best.pt* in the same\r\ndirectory where all of the corresponding checkpoints lie. This also compresses the file size slightly, so you should do\r\nthis and then use the *best.pt* model for inference.\r\n\r\n## Creating a new InferenceInterface\r\n\r\nTo build a new *InferenceInterface*, which you can then use for super simple inference, we're going to use an existing\r\none as template again. If you use multi-speaker, take the LibriTTS ones as template, otherwise take any other one. Make\r\na copy of the *InferenceInterface*. Change the name of the class in the copy and change the paths to the models to use\r\nthe trained models of your choice. Instantiate the model with the same hyperparameters that you used when you created it\r\nin the corresponding training pipeline. The last thing to check is the language that you supply to the text frontend.\r\nMake sure it matches what you used during training.\r\n\r\nWith your newly created *InferenceInterface*, you can use your trained models pretty much anywhere, e.g. in other\r\nprojects. All you need is the *Utility* directory, the *Layers* directory, the *PreprocessingForTTS* directory and \r\nthe *InferenceInterfaces* directory (and of course your model checkpoint). That's all the code you need, it works standalone.\r\n\r\n## Using a trained Model for Inference\r\n\r\nAn *InferenceInterface* of TransformerTTS contains 3 useful methods for inference, a FastSpeech 2 one contains 2 useful\r\nmethods. The ones they share are *read_to_file* and *read_aloud*.\r\n\r\n- The first one takes as input a list of strings and a filename. It will synthesize the sentences in the list and\r\n  concatenate them with a short pause inbetween and write them to the filepath you supply as the other argument.\r\n- The second one takes just a string, which it will then convert to speech and immediately play using the system's\r\n  speakers. If you set the optional argument *view* to *True* when calling it, it will also show a plot of the phonemes\r\n  it produced, the spectrogram it came up with and the wave it created from that spectrogram. So all of the\r\n  representations can be seen, text to phoneme, phoneme to spectrogram and finally spectrogram to wave.\r\n- The one that is exclusive to TransformerTTS is *plot_attentions*. This will take a string, synthesize it and show a\r\n  plot of the attention matrices of all of the attention heads in all of the attention layers. By default there are 24.\r\n  The index of each head is displayed above (0-23 by default). This is useful to figure out which one of the heads\r\n  contains the best temporal relation between text and audio, which is the ID that should be given to the FastSpeech 2\r\n  feature-extraction to heuristically get duration information using a technique called *knowledge-distillation*.\r\n\r\nThose three methods are used in demo code in the toolkit. In *run_interactive_demo.py*, *run_text_to_file_reader.py*\r\nand *run_visualizations.py* you can import *InferenceInterfaces* that you created and add them to the dictionary in each\r\nof the files with a shorthand that makes sense. In the interactive demo, you can just call the python script, then type\r\nin the shorthand when prompted and immediately listen to your synthesis saying whatever you put in next (be wary of out\r\nof memory errors for too long inputs). In the other two demo scripts you have to call the function that wraps around\r\nthe *InferenceInterface* and supply the shorthand of your choice. It should be pretty clear from looking at it.\r\n\r\n---\r\n\r\n## Example Pipelines available\r\n\r\n| Dataset               | Language  | Single or Multi     | MelGAN | TransformerTTS | FastSpeech2 | \r\n| ----------------------|-----------|---------------------| :-----:|:--------------:|:-----------:|\r\n| Hokuspokus            | German    | Single Speaker      | ✅     | ✅            | ✅          |\r\n| Thorsten              | German    | Single Speaker      | ✅     | ✅            | ✅          |\r\n| MAILabs Karlsson      | German    | Single Speaker      | ✅     | ✅            | ✅          |\r\n| MAILabs Eva           | German    | Single Speaker      | ✅     | ✅            | ✅          |\r\n| LJSpeech              | English   | Single Speaker      | ✅     | ✅            | ✅          |\r\n| Nancy Krebs Lessac    | English   | Single Speaker      | ✅     | ✅            | ✅          |\r\n| MAILabs Elizabeth     | English   | Single Speaker      | ✅     | ✅            | ✅          |\r\n| LibriTTS              | English   | Multi Speaker       | ✅     | ✅            | ✅          |\r\n\r\n---\r\n\r\nThis toolkit has been written by Florian Lux (except for the pytorch modules taken\r\nfrom [ESPnet](https://github.com/espnet/espnet) and [ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN),\r\nas mentioned above), so if you come across problems or questions, feel free to [write a mail](mailto:florian.lux@ims.uni-stuttgart.de). Also let me know if you do something\r\ncool with it. Thank you for reading.\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}
